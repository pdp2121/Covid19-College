{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import langdetect\n",
    "import pandas as pd\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "from ekphrasis.dicts.noslang.slangdict import slangdict\n",
    "import re\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter_2018 - 1grams ...\n",
      "Reading twitter_2018 - 2grams ...\n",
      "Reading twitter_2018 - 1grams ...\n",
      "Reading twitter_2018 - 1grams ...\n",
      "Reading twitter_2018 - 2grams ...\n"
     ]
    }
   ],
   "source": [
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user', 'time', 'date'],\n",
    "    # terms that will be annotated\n",
    "    annotate=['hashtag', 'allcaps', 'elongated', 'repeated', 'emphasis', 'censored'],\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used\n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter_2018\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter_2018\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=True,  # spell correction for elongated words\n",
    "    spell_correction=True,\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons, slangdict]\n",
    "    )\n",
    "\n",
    "segmenter=Segmenter(corpus=\"twitter_2018\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_words(sent, words):\n",
    "    for w in words:\n",
    "        if w in sent:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\",\", \" \", text)\n",
    "    text = re.sub(\":\", \"\", text)\n",
    "    text = re.sub(\"@ \", \"@\", text)\n",
    "    text = re.sub(\"# \", \"#\", text)\n",
    "    text = re.sub('\\S*@\\S*\\s?', '', text)\n",
    "    text = re.sub(r\"\\\"\", \"\", text)\n",
    "    text = re.sub(\"\\'\", \"\", text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r'\\w*pic.twitter.co\\w*', '', text)\n",
    "    text = re.sub(r'\\w*twitter.co\\w*', '', text)\n",
    "    text = re.sub(r'\\w*twitter.com\\w*', '', text)\n",
    "    text = re.sub(r\"./\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@ \\S+\", \"\", text)\n",
    "    text = re.sub(r\"#\\S+\", \"\", text)\n",
    "    text = re.sub(r'\\n+', \" \", text)\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    text = re.sub(\"co vid\", \"covid\", text)\n",
    "    text = re.sub(r\"\\ss\\s\", \" 's \", text)\n",
    "    text = re.sub(r\"\\sm\\s\", \" 'm \", text)\n",
    "    text = re.sub(r\"\\sll\\s\", \" 'll \", text)\n",
    "    text = re.sub(r\"\\st\\s\", \" 't \", text)\n",
    "    text = re.sub(r\"\\sd\\s\", \" 'd \", text)\n",
    "    text = re.sub(r\"\\svir\\s\", \" virus \", text)\n",
    "    text = re.sub(\"rt\", \"\", text)\n",
    "    try:\n",
    "        if langdetect.detect(text) == 'en':\n",
    "            return text\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "    return text\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1873022\n"
     ]
    }
   ],
   "source": [
    "users = json.load(open('covid_users_100000_non-org.jsonl', 'r'))\n",
    "tweet_count = 0\n",
    "for user in users:\n",
    "    tweet_file = './data/user-timelines/'+user['id_str']+'_tweets.jsonl'\n",
    "    try:\n",
    "        tweets = open(tweet_file, 'r').readlines()\n",
    "        tweets = [json.loads(t) for t in tweets]\n",
    "        tweet_count += len(tweets)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(tweet_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"covid\", \"covid_19\", \"coronavirus\", \"corona\", \"covid-19\", \"corona virus\",\n",
    "            \"covid\", \"chinesevirus\", \"chinese virus\", \"chinese_virus\", \"chinese-virus\"]\n",
    "\n",
    "processed_tweets = []\n",
    "\n",
    "for user in users:\n",
    "    tweet_file = './data/user-timelines/'+user['id_str']+'_tweets.jsonl'\n",
    "    try:\n",
    "        tweets = open(tweet_file, 'r').readlines()\n",
    "        tweets = [json.loads(t) for t in tweets]\n",
    "        for tweet in tweets:\n",
    "            if contains_words(tweet['text'].lower(), keywords):\n",
    "                tweet = tweet.encode('ascii', 'ignore').decode('utf-8')\n",
    "                tokens = text_processor.pre_process_doc(tweet['text'])\n",
    "                tokens = [segmenter.segment(t) for t in tokens]\n",
    "                text = \" \".join(tokens)\n",
    "                tweet_dict = dict()\n",
    "                tweet_dict['id'] = tweet['id']\n",
    "                tweet_dict['user'] = tweet['user']['id']\n",
    "                tweet_dict['text'] = preprocess(text)\n",
    "                if tweet_dict['text']:\n",
    "                    processed_tweets.append(tweet_dict)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "print('Processed {} tweets.'.format(len(processed_tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"id\": 1240281265322033154,\n",
      "    \"user\": 889861054850891776,\n",
      "    \"text\": \"   revelation  11   6 these men have power .  to strike the earth with every kind of plague as often as they want .   corona virus  \\u2026\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 1240281153921339393,\n",
      "    \"user\": 889861054850891776,\n",
      "    \"text\": \"   we do all we can but these are the beginning of birth pains . our focus is in heaven .  lock down kenya   coronavirus outbreak   c  \\u2026\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 1240014758578925570,\n",
      "    \"user\": 38333629,\n",
      "    \"text\": \"what happens if we have to plan a funeral during a pandemic like corona virus ? scott let 's you know what mueller mem \\u2026 \"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 1240548736763408384,\n",
      "    \"user\": 66330918,\n",
      "    \"text\": \"parents were moaning saying schools should close   now they have   they \\u2019 reply moaning about them closing .   covid  19 uk   school closures uk \"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 1240317440871014402,\n",
      "    \"user\": 66330918,\n",
      "    \"text\": \"   the news has been pretty occupied with coronavirus lately   so you may have missed this piece of good news  as of march \\u2026\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(processed_tweets[:5], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import logging\n",
    "\n",
    "class StanfordNLP:\n",
    "    def __init__(self, host='http://localhost', port=9000):\n",
    "        self.nlp = StanfordCoreNLP(host, port=port, timeout=30000)  # , quiet=False, logging_level=logging.DEBUG)\n",
    "        self.props = {\n",
    "            'annotators': 'tokenize,ssplit,pos,lemma,ner,parse,depparse,dcoref,relation',\n",
    "            'pipelineLanguage': 'en',\n",
    "            'outputFormat': 'json'\n",
    "        }\n",
    "\n",
    "    def word_tokenize(self, sentence):\n",
    "        return self.nlp.word_tokenize(sentence)\n",
    "\n",
    "    def pos(self, sentence):\n",
    "        return self.nlp.pos_tag(sentence)\n",
    "\n",
    "    def ner(self, sentence):\n",
    "        return self.nlp.ner(sentence)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        return self.nlp.parse(sentence)\n",
    "\n",
    "    def dependency_parse(self, sentence):\n",
    "        return self.nlp.dependency_parse(sentence)\n",
    "\n",
    "    def annotate(self, sentence):\n",
    "        return json.loads(self.nlp.annotate(sentence, properties=self.props))\n",
    "\n",
    "    @staticmethod\n",
    "    def tokens_to_dict(_tokens):\n",
    "        tokens = defaultdict(dict)\n",
    "        for token in _tokens:\n",
    "            tokens[int(token['index'])] = {\n",
    "                'word': token['word'],\n",
    "                'lemma': token['lemma'],\n",
    "                'pos': token['pos'],\n",
    "                'ner': token['ner']\n",
    "            }\n",
    "        return tokens\n",
    "\n",
    "nlp = StanfordNLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in processed_tweets:\n",
    "    tweet['lemma'] = \"\"\n",
    "    tweet['pos'] = \"\"\n",
    "    for sent in nlp.annotate(tweet['text'])['sentences']:\n",
    "        for token in sent['tokens']:\n",
    "            tweet['lemma'] = \" \".join([tweet['lemma'], token['lemma']])\n",
    "            tweet['pos'] = \" \".join([tweet['pos'], token['pos']])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"id\": 1240281265322033154,\n",
      "    \"user\": 889861054850891776,\n",
      "    \"text\": \"   revelation  11   6 these men have power .  to strike the earth with every kind of plague as often as they want .   corona virus  \\u2026\",\n",
      "    \"lemma\": \" revelation 11 6 these man have power . to strike the earth with every kind of plague as often as they want . corona virus ...\",\n",
      "    \"pos\": \" NN CD CD DT NNS VBP NN . TO VB DT NN IN DT NN IN VB RB RB IN PRP VBP . NN NN :\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 1240281153921339393,\n",
      "    \"user\": 889861054850891776,\n",
      "    \"text\": \"   we do all we can but these are the beginning of birth pains . our focus is in heaven .  lock down kenya   coronavirus outbreak   c  \\u2026\",\n",
      "    \"lemma\": \" we do all we can but these be the beginning of birth pain . we focus be in heaven . lock down kenya coronavirus outbreak c ...\",\n",
      "    \"pos\": \" PRP VBP DT PRP MD CC DT VBP DT NN IN NN NNS . PRP$ NN VBZ IN NN . VB RP NN NN NN NN :\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 1240014758578925570,\n",
      "    \"user\": 38333629,\n",
      "    \"text\": \"what happens if we have to plan a funeral during a pandemic like corona virus ? scott let 's you know what mueller mem \\u2026 \",\n",
      "    \"lemma\": \" what happen if we have to plan a funeral during a pandemic like corona virus ? scott let 's you know what mueller mem ...\",\n",
      "    \"pos\": \" WP VBZ IN PRP VBP TO VB DT NN IN DT JJ IN NN NN . NN VBD POS PRP VBP WP NN NN :\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 1240548736763408384,\n",
      "    \"user\": 66330918,\n",
      "    \"text\": \"parents were moaning saying schools should close   now they have   they \\u2019 reply moaning about them closing .   covid  19 uk   school closures uk \",\n",
      "    \"lemma\": \" parent be moan say school should close now they have they ' reply moan about they close . covid 19 uk school closure uk\",\n",
      "    \"pos\": \" NNS VBD VBG VBG NNS MD RB RB PRP VBP PRP '' NN VBG IN PRP VBG . NN CD NN NN NNS VBP\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 1240317440871014402,\n",
      "    \"user\": 66330918,\n",
      "    \"text\": \"   the news has been pretty occupied with coronavirus lately   so you may have missed this piece of good news  as of march \\u2026\",\n",
      "    \"lemma\": \" the news have be pretty occupy with coronavirus lately so you may have miss this piece of good news as of march ...\",\n",
      "    \"pos\": \" DT NN VBZ VBN RB VBN IN NN RB RB PRP MD VB VBN DT NN IN JJ NN IN IN NN :\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(processed_tweets[:5], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(directory+'/covid_tweets_100000.json', 'w') as f:\n",
    "    json.dump(processed_tweets, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
